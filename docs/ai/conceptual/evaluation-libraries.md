---
title: The Microsoft.Extensions.AI.Evaluation libraries
description:
ms.topic: concept-article
ms.date: 02/14/2025
---
# The Microsoft.Extensions.AI.Evaluation libraries (Preview)

*Evaluation* refers to the process of assessing the quality and accuracy of responses generated by AI models. Various metrics measure aspects like relevance, truthfulness, coherence, and completeness of the responses. Evaluations are crucial in testing because they help ensure that the AI model performs as expected, which is to provide reliable and accurate results that enhance user experience and satisfaction.

## Packages

Built on top of the [Microsoft.Extensions.AI abstractions](../ai-extensions.md), the Microsoft.Extensions.AI.Evaluation libraries (currently in preview) simplify the process of evaluating the quality and accuracy of your .NET intelligent apps. The following NuGet packages comprise the libraries:

- [ðŸ“¦ Microsoft.Extensions.AI.Evaluation](https://www.nuget.org/packages/Microsoft.Extensions.AI.Evaluation) â€“ Defines the core abstractions and types for supporting evaluation.
- [ðŸ“¦ Microsoft.Extensions.AI.Evaluation.Quality](https://www.nuget.org/packages/Microsoft.Extensions.AI.Evaluation.Quality) â€“ Contains evaluators that assess the quality of LLM responses in an app according to metrics such as relevance, fluency, coherence, and truthfulness.
- [ðŸ“¦ Microsoft.Extensions.AI.Evaluation.Reporting](https://www.nuget.org/packages/Microsoft.Extensions.AI.Evaluation.Reporting) â€“ Contains support for caching LLM responses, storing the results of evaluations, and generating reports from that data.
- [ðŸ“¦ Microsoft.Extensions.AI.Evaluation.Console](https://www.nuget.org/packages/Microsoft.Extensions.AI.Evaluation.Console) â€“ A command-line tool for generating reports and managing evaluation data.

## Test integration

The libraries are designed to integrate smoothly with existing .NET apps, allowing you to leverage existing testing infrastructures and familiar syntax to evaluate intelligent apps. You can use any test framework (for example, [MSTest](../../core/testing/index.md#mstest), [xUnit](../../core/testing/index.md#xunit), or [NUnit](../../core/testing/index.md#nunit)) and testing workflow (for example, Test Explorer, [dotnet test](../../core/tools/dotnet-test.md), or a CI/CD pipeline). The library also provides easy ways to do online evaluations of your application by publishing evaluation scores to telemetry and monitoring dashboards.

## Comprehensive evaluation metrics

The evaluation libraries were built in collaboration with data science researchers from Microsoft and GitHub, and were tested on popular Microsoft Copilot experiences. They provide built-in evaluators for:

- Relevance (how effectively a response addresses a query)
- Truth
- Completeness
- Fluency (grammatical accuracy, vocabulary range, sentence complexity, and overall readability)
- Coherence (the logical and orderly presentation of ideas)
- Equivalence (the similarity between the generated text and its ground truth with respect to a query)
- Groundedness (how well a generated response aligns with the given context)

You can also customize to add your own evaluations by implementing the <xref:Microsoft.Extensions.AI.Evaluation.IEvaluator> interface.

## Cached responses

The library uses *response caching* functionality, which means responses from the AI model are persisted in a cache. In subsequent runs, if the request parameters (prompt and model) are unchanged, responses are then served from the cache to enable faster execution and lower cost.

## Configuration

The libraries are designed to be flexible. You can pick the components that you need. For example, you can disable response caching or tailor reporting to work best in your environment. You can also customize and configure your evaluations, for example, by adding customized metrics and reporting options.

## Samples

For a more comprehensive tour of the functionality and APIs available in the Microsoft.Extensions.AI.Evaluation libraries, see the [API usage examples (dotnet/ai-samples repo)](https://github.com/dotnet/ai-samples/blob/main/src/microsoft-extensions-ai-evaluation/api/). These examples are structured as a collection of unit tests. Each unit test showcases a specific concept or API and builds on the concepts and APIs showcased in previous unit tests.

## See also

- [Evaluation of generative AI apps (Azure AI Foundry)](/azure/ai-studio/concepts/evaluation-approach-gen-ai)
