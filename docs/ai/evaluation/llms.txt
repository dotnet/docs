# Evaluation Docs

> Measuring and validating AI response quality using automated evaluation frameworks. The `Microsoft.Extensions.AI.Evaluation` library provides scoring metrics for relevance, coherence, and correctness. Use response caching and reporting for offline evaluation pipelines in unit tests.

- [Quickstart](https://raw.githubusercontent.com/dotnet/docs/refs/heads/llmstxt/docs/ai/evaluation/evaluate-ai-response.md): Learn how to create an MSTest app to evaluate the AI chat response of a language model.
- [Tutorial](https://raw.githubusercontent.com/dotnet/docs/refs/heads/llmstxt/docs/ai/evaluation/evaluate-safety.md): Create an MSTest app that evaluates the content safety of a model's response using the evaluators in the Microsoft.Extensions.AI.Evaluation.Safety package and with caching and reporting.
- [Tutorial](https://raw.githubusercontent.com/dotnet/docs/refs/heads/llmstxt/docs/ai/evaluation/evaluate-with-reporting.md): Create an MSTest app to evaluate the response quality of a language model, add a custom evaluator, and learn how to use the caching and reporting features of Microsoft.Extensions.AI.Evaluation.
- [The Microsoft.Extensions.AI.Evaluation libraries](https://raw.githubusercontent.com/dotnet/docs/refs/heads/llmstxt/docs/ai/evaluation/libraries.md): Learn about the Microsoft.Extensions.AI.Evaluation libraries, which simplify the process of evaluating the quality and accuracy of responses generated by AI models in .NET intelligent apps.
- [Responsible AI with .NET](https://raw.githubusercontent.com/dotnet/docs/refs/heads/llmstxt/docs/ai/evaluation/responsible-ai.md): Learn what responsible AI is and how you can use .NET to evaluate the safety of your AI apps.
